# As a Product Manager

Once we identified a business problem to solve with LLMs, a few natural places to integrate LLMs emerged. We had to make some product decisions on what parts of the user journey to enhance with LLMs.

## Onboarding

LLM-generated output is a great way to solve the cold-start problem users face on your platform.&#x20;

For us, that was “what SQL query can I write on my data”. By adding a tab to ask our AI to suggest queries and data trends to explore while users were onboarding, we improved conversion and stickiness of downstream features.

<figure><img src="../.gitbook/assets/LogicLoop AI SQL Copilot.gif" alt="" width="375"><figcaption></figcaption></figure>

## 10x Asset Creation

LLM-generated output is also great at helping your users work through your core product quickly and efficiently.&#x20;

We added our AI SQL Copilot right on the page where users were creating, modifying and saving their SQL queries for maximum impact.

<figure><img src="../.gitbook/assets/LogicLoop Generate Query.gif" alt="" width="275"><figcaption></figcaption></figure>

## Debugging

The more constrained inputs to your LLMs can be, the less you run into hallucinations and other typical issues. This makes LLMs good debugging assistants.

We've had success integrating LLMs into workflows where we expect the user to be debugging SQL queries as part of the AI SQL Copilot. It's the perfect combination of being too frustrating for a human to figure out where they missed a semi-colon and a machine being able to do that easily.&#x20;

<figure><img src="../.gitbook/assets/LogicLoop Fix Query.gif" alt=""><figcaption></figcaption></figure>

